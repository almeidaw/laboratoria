{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visão Geral\n",
    "---\n",
    "\n",
    "## Usando um Jupyter Notebook\n",
    "\n",
    "Para esta parte do tutorial Mythical Mysfits, você estará interagindo diretamente com este Jupyter notebook para construir um modelo de machine learning usando dados de amostra que nós fornecemos. Um notebook oferece a capacidade de criar uma documentação rica ao lado do código e de sua saída, ajudando a descrever e executar as etapas realizadas em direção aos seus objetivos de aprendizado de máquina em um único local.\n",
    "\n",
    "Cada parte independentemente executável de um notebook é representada como uma **célula** distinta. Uma célula pode ser de um dos três tipos:\n",
    "* Markdown (rich text)\n",
    "* Code (Python nesse notebook)\n",
    "* Raw (conteúdos usados ​​diretamente da saída da própria célula, nota usada neste caderno)\n",
    "\n",
    "Se você clicar em torno deste documento de notebook em várias partes dele, você verá uma borda realçar uma parte do documento, que representa uma célula.\n",
    "\n",
    "Selecionar uma célula e clicar no botão **Run** na barra de ferramentas executará essa célula, assim como pressionar Ctrl+Enter no teclado. Para uma célula Markdown, isso formatará o texto escrito e o exibirá de acordo com a sintaxe de Markdown. Para uma célula Code, o código Python será executado no kernel subjacente e a saída do código será exibida sob a célula.\n",
    "\n",
    "Para as células Code, você pode notar `In []:` à direita de cada célula. Isso é usado para indicar o status de execução e a seqüência de cada bloco de código dentro do notebook. Os colchetes vazios (`[]`) indicam que um bloco de código ainda possui uma nota sendo executada. Quando um bloco de código está no meio de sua execução, mas ainda não a completou, você verá `[*]` ser exibido. E finalmente, uma vez que um bloco de código tenha terminado a execução, você verá um número específico exibido como `[1]`. Este número representa a seqüência em que esse bloco de código foi executado em relação àqueles antes dele dentro do notebook. Isso é para ajudá-lo a acompanhar o estado atual da execução do código na totalidade do documento do notebook (como você pode executar uma célula e, em seguida, ler alguma documentação e não lembrar exatamente qual célula foi executada pela última vez!).\n",
    "\n",
    "Se você precisar reverter o processamento por qualquer motivo, você pode usar o menu ** Kernel ** acima na barra de ferramentas do notebook para resetar o kernel, limpar o output, etc.\n",
    "\n",
    "## O Notebook de Recomendações de Mysfits\n",
    "\n",
    "O código necessário para usar os dados de amostra e criar um modelo de machine learning já foi escrito e está contido nas seguintes células abaixo neste notebook. É sua tarefa ler a documentação para entender as etapas realizadas e familiarizar-se com a interação com este notebook para organizar dados, construir e treinar o modelo de machine learning e implantar esse modelo para uso de nossa aplicação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: Fazendo o Download dos Dados de Amostra\n",
    "---\n",
    "A célula de código abaixo faz o download dos dados de amostra que foram testados no S3.\n",
    "\n",
    "O conjunto de dados contém as respostas a um questionário de quase um milhão de usuários imaginários do site Mythical Mysfits e qual é o seu Mysfit favorito. Para casos de uso como este, em que o algoritmo utilizado espera entradas numéricas, mapeamos cada possível resposta do questionário e o mysfit escolhido para um valor numérico. O resultado do questionário de cinco perguntas e um mysfit favorito é um arquivo CSV onde cada linha contém 6 valores separados por vírgula (Exemplo: `1,0,2,7,0,11`).  Por favor visite o [Website do Mythical Mysfits](http://www.mythicalmysfits.com) para testar o questionário você mesmo.\n",
    "\n",
    "Clique na célula de código abaixo para delimitá-la e, em seguida, clique em **Run** acima na barra de ferramentas ou pressione Ctrl+Enter no seu teclado para baixar o conjunto de dados de amostra e armazená-lo no diretório listado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget 'https://s3.amazonaws.com/mysfit-recommendation-training-data/mysfit-preferences.csv.gz'\n",
    "mkdir -p /tmp/mysfit/raw\n",
    "mv mysfit-preferences.csv.gz /tmp/mysfit/raw/mysfit-preferences.csv.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Parte 2: Preparação dos dados\n",
    "---\n",
    "\n",
    "## Pré-Processando os Dados\n",
    "Agora que temos os dados brutos, vamos processá-los. \n",
    "Primeiro, carregaremos os dados em vetores numpy e os dividiremos aleatoriamente em treino e teste com uma divisão 90/10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a58fdf0d-32fb-4690-add3-433cc721773d"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_dir = \"/tmp/mysfit/\"\n",
    "processed_subdir = \"standardized\"\n",
    "raw_data_file = os.path.join(data_dir, \"raw\", \"mysfit-preferences.csv.gz\")\n",
    "train_features_file = os.path.join(data_dir, processed_subdir, \"train/csv/features.csv\")\n",
    "train_labels_file = os.path.join(data_dir, processed_subdir, \"train/csv/labels.csv\")\n",
    "test_features_file = os.path.join(data_dir, processed_subdir, \"test/csv/features.csv\")\n",
    "test_labels_file = os.path.join(data_dir, processed_subdir, \"test/csv/labels.csv\")\n",
    "\n",
    "# read raw data\n",
    "print(\"Reading raw data from {}\".format(raw_data_file))\n",
    "raw = np.loadtxt(raw_data_file, delimiter=',')\n",
    "\n",
    "# split into train/test with a 90/10 split\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(raw)\n",
    "train_size = int(0.9 * raw.shape[0])\n",
    "train_features = raw[:train_size, :-1]\n",
    "train_labels = raw[:train_size, -1]\n",
    "test_features = raw[train_size:, :-1]\n",
    "test_labels = raw[train_size:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo o upload para o Amazon S3\n",
    "Agora, como normalmente o conjunto de dados será grande e está localizado no Amazon S3, vamos gravar os dados no Amazon S3 no formato recordio-protobuf. Primeiro, criamos um buffer de io envolvendo os dados, em seguida fazemos o upload para o Amazon S3. Observe que a escolha do bucket e prefixo deve mudar para usuários diferentes e conjuntos de dados diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "print('train_features shape = ', train_features.shape)\n",
    "print('train_labels shape = ', train_labels.shape)\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, train_features, train_labels)\n",
    "buf.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket() # modify to your bucket name\n",
    "prefix = 'mysfit-recommendation-dataset'\n",
    "key = 'recordio-pb-data'\n",
    "\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também é possível fornecer dados de teste. Dessa forma, podemos obter uma avaliação do desempenho do modelo a partir dos logs de treinamento. Para usar esse recurso, vamos fazer o upload dos dados de teste para o Amazon S3 também"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_features shape = ', test_features.shape)\n",
    "print('test_labels shape = ', test_labels.shape)\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, test_features, test_labels)\n",
    "buf.seek(0)\n",
    "\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test', key)).upload_fileobj(buf)\n",
    "s3_test_data = 's3://{}/{}/test/{}'.format(bucket, prefix, key)\n",
    "print('uploaded test data location: {}'.format(s3_test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3: Treinamento\n",
    "---\n",
    "\n",
    "Levamos um momento para explicar em alto nível como treinamento e previsão de Machine Learning funcionam no Amazon SageMaker. Primeiro, precisamos treinar um modelo. Este é um processo que, dado um conjunto de dados e hiperparâmetros rotulados, guiando o processo de treinamento, gera um modelo. Uma vez que o treinamento é feito, nós configuramos o que é chamado de ** endpoint **. Um endpoint é um serviço web que, dada uma requisição contendo um datapoint não rotulado, ou mini-batch de datapoint, retorna uma predição (ou predições).\n",
    "\n",
    "No Amazon SageMaker, o treinamento é feito através de um objeto chamado **estimator**. Ao configurar o estimator, especificamos a localização (no Amazon S3) dos dados de treinamento, o caminho (novamente no Amazon S3) para o diretório de saída onde o modelo será serializado, hiperparâmetros genéricos como o tipo de máquina a ser usado durante o processo de treinamento, e hiperparâmetros específicos para kNN, como o tipo de índice, etc. Uma vez que o estimator é inicializado, podemos chamar o seu método **fit** para fazer o treinamento real.\n",
    "\n",
    "Agora que estamos prontos para o treinamento, começamos com uma função de conveniência que inicia um job de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "\n",
    "def trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path, s3_test_data=None):\n",
    "    \"\"\"\n",
    "    Create an Estimator from the given hyperparams, fit to training data, \n",
    "    and return a deployed predictor\n",
    "    \n",
    "    \"\"\"\n",
    "    # set up the estimator\n",
    "    knn = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, \"knn\"),\n",
    "        get_execution_role(),\n",
    "        train_instance_count=1,\n",
    "        train_instance_type='ml.m5.2xlarge',\n",
    "        output_path=output_path,\n",
    "        sagemaker_session=sagemaker.Session())\n",
    "    knn.set_hyperparameters(**hyperparams)\n",
    "    \n",
    "    # train a model. fit_input contains the locations of the train and test data\n",
    "    fit_input = {'train': s3_train_data}\n",
    "    if s3_test_data is not None:\n",
    "        fit_input['test'] = s3_test_data\n",
    "    knn.fit(fit_input)\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, nós executamos o job real de treinamento. Por enquanto, nos atemos aos parâmetros padrão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'feature_dim': 5,\n",
    "    'k': 10,\n",
    "    'sample_size': 100000,\n",
    "    'predictor_type': 'classifier' \n",
    "}\n",
    "output_path = 's3://' + bucket + '/' + prefix + '/default_example/output'\n",
    "knn_estimator = trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path, \n",
    "                                                   s3_test_data=s3_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que mencionamos um conjunto de testes no job de treinamento. Quando um conjunto de testes é fornecido, o job de treinamento não apenas produz um modelo, mas também o aplica ao conjunto de testes e informa a precisão. Nos logs, você pode ver a precisão do modelo no conjunto de testes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 4: Implantando o modelo em um Endpoint do SageMaker \n",
    "---\n",
    "\n",
    "## Configurando o endpoint\n",
    "\n",
    "Agora que temos um modelo treinado, estamos prontos para executar a inferência. O objeto **knn_estimator** acima contém todas as informações necessárias para hospedar o modelo. Abaixo, fornecemos uma função de conveniência que, dado um estimador, configura o endpoint que hospeda o modelo. Além do objeto estimador, fornecemos um nome (string) para o estimador e um **instance_type**. O **instance_type** é o tipo de máquina que hospedará o modelo. Ele não é restrito de forma alguma pelas configurações de parâmetros do job de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor_from_estimator(knn_estimator, estimator_name, instance_type, endpoint_name=None): \n",
    "    knn_predictor = knn_estimator.deploy(initial_instance_count=1, instance_type=instance_type,\n",
    "                                        endpoint_name=endpoint_name)\n",
    "    knn_predictor.content_type = 'text/csv'\n",
    "    knn_predictor.serializer = csv_serializer\n",
    "    knn_predictor.deserializer = json_deserializer\n",
    "    return knn_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "instance_type = 'ml.m4.xlarge'\n",
    "model_name = 'knn_%s'% instance_type\n",
    "endpoint_name = 'knn-ml-m4-xlarge-%s'% (str(time.time()).replace('.','-'))\n",
    "print('setting up the endpoint..')\n",
    "predictor = predictor_from_estimator(knn_estimator, model_name, instance_type, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferência\n",
    "\n",
    "Agora que temos nosso preditor, vamos usá-lo em nosso conjunto de dados de teste. O código a seguir é executado no conjunto de dados de teste, calcula a precisão e a latência média. Divide os dados em 100 batches. Em seguida, cada batch é fornecido ao serviço de inferência para obter predições. Assim que tivermos todas as predições, calculamos a sua precisão, dados os rótulos verdadeiros do conjunto de testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batches = np.array_split(test_features, 100)\n",
    "print('data split into 100 batches, of size %d.' % batches[0].shape[0])\n",
    "# obtain an np array with the predictions for the entire test set\n",
    "start_time = time.time()\n",
    "predictions = []\n",
    "for batch in batches:\n",
    "    result = predictor.predict(batch)\n",
    "    cur_predictions = np.array([result['predictions'][i]['predicted_label'] for i in range(len(result['predictions']))])\n",
    "    predictions.append(cur_predictions)\n",
    "predictions = np.concatenate(predictions)\n",
    "run_time = time.time() - start_time\n",
    "\n",
    "test_size = test_labels.shape[0]\n",
    "num_correct = sum(predictions == test_labels)\n",
    "accuracy = num_correct / float(test_size)\n",
    "print('time required for predicting %d data point: %.2f seconds' % (test_size, run_time))\n",
    "print('accuracy of model: %.1f%%' % (accuracy * 100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: Lembre-se de que esse conjunto de dados de amostra foi gerado aleatoriamente. Portanto, você notará a baixa precisão que esse modelo é capaz de alcançar (porque há pouco padrão nos dados que estão sendo usados ​​para criar o modelo). \n",
    "\n",
    "Para os seus casos de uso futuros usando o aprendizado de máquina e o SageMaker, cabe a você determinar o nível de precisão exigido para que o modelo seja benéfico para sua aplicação. Nem todos os casos de uso exigem mais de 90% de precisão para que os benefícios sejam obtidos. Embora, para alguns casos de uso, especialmente quando a segurança do cliente fizer parte de sua aplicação, você pode determinar que um modelo deve ter níveis extremos de precisão para ser utilizado em Produção."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARE!\n",
    "\n",
    "## Próximos Passos do Workshop Mythical Mysfits \n",
    "Você acaba de implantar um endpoint de predição para o SageMaker. Ele pode ser invocado via HTTP diretamente. No entanto, em vez de diretamente integrar o frontend de nossa aplicação com o endpoint nativo do SageMaker, vamos incluir nossa própria API RESTful e serverless em neste endpoint de previsão. Por favor retorne às instruções do workshop e prossiga para o próximo passo para continuar o tutorial. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Clean-Up quando Completar o Módulo 7\n",
    "\n",
    "## Deletando o endpoint\n",
    "\n",
    "Agora terminamos com o exemplo, exceto um ato final de clean-up. Ao configurar o endpoint, iniciamos uma máquina na nuvem e, enquanto não for excluída, a máquina ainda estará funcionando e estamos pagando por ela. Uma vez que o endpoint não é mais necessário, nós o deletamos. O código a seguir faz exatamente isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_endpoint(predictor):\n",
    "    try:\n",
    "        boto3.client('sagemaker').delete_endpoint(EndpointName=predictor.endpoint)\n",
    "        print('Deleted {}'.format(predictor.endpoint))\n",
    "    except:\n",
    "        print('Already deleted: {}'.format(predictor.endpoint))\n",
    "\n",
    "delete_endpoint(predictor)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
